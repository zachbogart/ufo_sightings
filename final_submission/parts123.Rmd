---
title: "EDAV Project: UFO Sightings"
author: "Josh Feldman (jbf2159), Ramy Jaber (rij2105), Robert Davis DeRodes (rd2804), Zach Bogart (zb2223)"
date: "4/5/2018"
output: html_document
---

# 1 Introduction

Humans yearn to give meaning to anything and everything. When it was discovered that Earth was a planet existing in a larger space, it was immediately theorized that there could be alien life in this unexplored space. To this day, there is constant thought and theorizing about the existence of extraterrestrial life, from scientific papers backed by observational evidence to explosive films exploring imaginary invasions. The data we are exploring contains documented UFO sightings. When someone sees something in the sky they cannot explain, they can call and report a sighting or simply fill out a form online. If aliens constantly visit our planet, they would likely have a clear pattern they use to choose target locations and times.  We want to explore this data to try and extract patterns from the randomness. 

The main questions we wanted to answer were:

* Do different shapes of UFOs behave differently?  
* Have UFO patterns remained constant over time? (This dataset has enough observations since the 1950’s to answer these questions)
* Where do UFO sightings occur?
* Is there insight into the reasons why these patterns exist?

What we did:

* Josh: D3 interactive, EDA
* Ramy: D3 interactive, report cleanup, data scraping
* Davis: EDA, executive summary, research/other datasets
* Zach: EDA, data cleaning, writeup


# 2 Description of Data

Our data was scraped off of the National UFO Reporting Center’s (NUFORC) website’s event summary page (http://www.nuforc.org/webreports/ndxevent.html) on March 31, 2018. The NUFORC’s primary channel through which they receive sightings is via their telephone hotline, which is staffed 24 hours a day. The NUFORC also receives sighting reports including images and video via email. They recommend that a summary of the event be written in a pre-specified form they have available on their website. The NUFORC is headquartered in Davenport, WA (East Washington). 

The events summary page has events that go back as far as 1561 (where the artist Hans Glaser depicted his UFO sighting in wood) until March 2018. The reports come from a variety of countries, but the large majority of them are from the United States. Each month's summary page has summaries of all of their reported sightings including the Date/Time, City, State (if applicable), Shape of the UFO, Duration, text summary of the event, and the date it was posted on the website.  In several of the sightings’ summaries there are indications on whether or not the in-taker thought the sighting was a hoax or not. 

#### Data Scraping
We developed a python script to scrape the data from http://www.nuforc.org/webreports/. There is one index page for each month that contains links to detailed description of each UFO sighting. Using a html parsing package named BeautifulSoup, the script goes through each of these links to extract the data fields including date/time of observation, shape, city, state, duration, and full description. The source code can be viewed as an [html file](https://github.com/zachbogart/ufo_sightings/blob/master/final_submission/getUfoData.html) or as an [iPython notebook](Source code can be viewed as an [html file|https://github.com/zachbogart/ufo_sightings/blob/master/final_submission/getUfoData.ipynb])

#### Two-word affinity
We wanted to explore how common descriptors are used together in a description of a UFO observation. To do so, we implemented a Word Count program using Spark and Python. To normalize the data and address grammatical differences between words (i.e. light and lights), we used stop words and stemming procedures from the natural language processing library NLTK. Then we calculate the frequency of every word across all descriptions. Then, we count how often two words appear in the same description. Source code can be viewed as an [html file](https://github.com/zachbogart/ufo_sightings/blob/master/final_submission/common_words_python.html) or as an [iPython notebook](Source code can be viewed as an [html file|https://github.com/zachbogart/ufo_sightings/blob/master/final_submission/common_words_python.ipynb]). This data is visualized in the d3 force diagram (see Part 6).

#### External Dataset
To better explore our theories behind UFO sighting locations, we used several external datasets. Our data for how many hours a day people at the state level spend on sports, exercise, and recreation and the percentage of people in a state that engaged in sports or exercise in a given day came from the American Time Use Survey. The ATUS is conducted by the Bureau of Labor Statistics. The data in the survey we used was collected from 2009-2015 via phone interviews in which interviewees were asked to recount the activities of their day. http://www.governing.com/topics/urban/gov-americans-time-use-survey-2015.html
https://www.bls.gov/spotlight/2017/sports-and-exercise/home.htm

Our data on state level obesity came from https://stateofobesity.org/adult-obesity/, an American watchdog organization on obesity in the United States. State of Obesity received their data from the CDC’s Behavioral Risk Factor Surveillance System dataset, a cross sectional study conducted over the phone. 
https://stateofobesity.org/adult-obesity/

The results by state of the 2016 election were accessed from https://www.politico.com/mapdata-2016/2016-election/results/map/president/ . 

# 3 Analysis of Data Quality

In this section we look at the data in its raw form and see what we will need to do in order to fix it for analysis. All of the cleaning of the data is done in `part4_cleaning.Rmd`. Here we are just assessing the ingredients before we go and bake up some tasty graphs and tables.

Because we scraped the data off of a website, it started out in a rather raw form. Using a python script with Beautiful Soup, we were able to collect all of the UFO Sightings listed on the NUFORC site.

**Note: the data is zipped in the data folder when downloaded and must be unzipped in order to read**
```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = FALSE, eval = FALSE)
knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE, 
                      cache = TRUE)

library(Sleuth3)
library(tidyverse)

library(DAAG)

library(vcd)

library(ggplot2)
library(extracat)
library(scales)
library(micromapST)
library(statebins)
library(lubridate)
library(maps)
```

```{r pull-in-data}
# read in raw data
ufo_raw <- as_tibble(read_csv("data/ufo_data_final.csv"))

head(ufo_raw, 5)
```

Looking at the data, there is a lot of inconsistency that needs to be corrected. This stems from the fact that sightings are reported using an online form that does not enforce data type in collection. Looking at the NUFORC submission form (http://www.ufocenter.com/reportformsubmit.html), there are suggestions for how you should format the report, but they are not required nor enforced. For example, where it asks for "Number of Witnesses" I could type in "foo" and the site will accept the submission. This is evident in the results we see in the raw data.

## The Basics

First off, the dataset has `r nrow(ufo_raw)` reported sightings spanning centuries. 

```{r basic-histogram-and-row-count}
# print(nrow(ufo_raw))
library(scales)
ggplot(ufo_raw, aes(x = year)) + geom_histogram(bins = 40, fill='dodgerblue') + ggtitle("The Skewing by Old-Timey Sightings") + scale_y_continuous(name='Sightings (in thousands)', labels = c(20,40,60,80),breaks = c(20000,40000,60000,80000)) + geom_vline(xintercept = 1974, color="red") + theme_light() +xlab("Year")

```

The above graph looks so left skewed because of a few sightings that are reported to have occurred anywhere between 1400 and 1800. These sightings may need to be ignored in order to better understand the behavior of the dataset without skewing the results. There are just `r count(filter(ufo_raw, year<1900))`. Although these sightings are a consideration, they are a relatively small percentage of the total dataset. In fact, the vast majority of sightings occur after the founding the the NUFORC in 1974 (http://www.nuforc.org/General.html), which is a good sign for data quality. The founding of the NUFORC is shown in red on the above graph.

Another good sign is that the most sightings have no NA values. Further, of the reports that are missing some information, they tend to miss only one field, with the State being the most frequent missing feature.
```{r na-values}
visna(ufo_raw[, c('Duration','Shape','State','City','Desc','Summary','Date / Time','Posted' )], sort = "b")
```

We will now explore the individual feature quality and cleaning.

## States

Let's look at the number of states we have in the dataset.
```{r state-count}
nrow(table(ufo_raw$State))

sort(unique(ufo_raw$State))
```

Due to the data entry issues discussed earlier, and that reports were not limited to the United States, there are 70 unique state abbreviations. Listing out the state abbreviations in alphabetical order, we can see that some of these can easily be identified as Canadian territories (i.e. 'YK' and 'BC'). Some territories may also have multiple valid abbreviations. For example, 'YT' and 'YK' are both abbreviations for Yukon Territory (https://en.wikipedia.org/wiki/Canadian_postal_abbreviations_for_provinces_and_territories). Also, the eagle-eyed viewer will notice Florida and California have a handful of Camel-Case results (Fl and Ca) that will need to be dealt with as well. When doing geographic based analysis, we have to standardize these to the known geographies.

## Shape

The raw dataset reports 46 unique shapes. Listing these out, we can see that many shapes are similar in description.
```{r shape-view}
sort(unique(ufo_raw$Shape))
```

Here we see a lot of categories that can be merged just by converting all of the types to lowercase ("changing" and "Changing") or by combining different tenses ("changed" and "changing") or a combination of both ("triangle", "Triangle", and "TRIANGULAR"). 

We also see `r table(ufo_raw$Shape)['Unknown']` values that are "Unknown". While "Unknown" is a good dropdown category for a shape, `NA` makes more sense in our data analysis, since we will only consider the definitive shapes that were observed.

We still want to reduce the number of unique shapes to help find trends. We decide to bin the shapes into similar categories. For example, we include 'round', 'sphere', 'oval', and 'circle' in the "Circle" category.

## Duration

The duration field in the submission for is a free-form data entry field, stored as string. This gives reporters the opportunity to input any words they want and leads to an entertaining array of time interval units. 
Because of this 'flexibility' in the `duration` field, there are `r length(unique(ufo_raw$Duration))` different inputs for this dataset.

To enforce just how inconsistent the reporting data can be, let's look at some of the more bizarre measures of time.

```{r gems-duration}
print(ufo_raw$Duration[c(23,37,58,86,280,318,111828,1213,17679,65718,42052,31738)])
```

However, the majority follow a common structure which can be exploited. Most entries are a number, a space, and a time frame. For example, "45 seconds" or "5 minutes"
```{r common-formatting}
print(ufo_raw$Duration[2:8])
```

This was by far the messiest column to deal with. In order to convert the dataset, we write a set of functions to handle all of the corner cases and convert them into numbers on a common scale (in cleaning file).

Further, since there were many unique values of duration, we decided to bin into groups for some analyses. Groups included '<1 min', '1-5min', all the way to '>60 minutes'.

## Part 3 Summary

The primary takeaway from the raw data assessment is that data consistency is key, and should be enforced from data collection to reduce the cleaning process downstream. For example, free form fields should be left for comments or descriptions, while fields that have an inherent structure, should be enforced as such (i.e. duration in time units). As Data Scientists, we should not underestimate the ability for a user to write something crazy in the input box.


<!-- Big points: -->
<!-- - Freeform submission is NOT a good idea -->
<!-- - Standardization will make your data scientists happy later on -->
<!-- - Don't combine fields into one column (`State` should probably be stored as international and domestic) -->
<!-- - Don't be afraid to store an unknown value as `NA` -->


<!-- At this point, take a look at the `part4_cleaning` file to see how we standardized the data to make the EDA possible. -->








